# ğŸ§  Personal AI Assistant â€“ On-Prem Model & Data Server

This repository contains the **on-premises infrastructure** for hosting AI models and data sources in a distributed, self-contained environment. Designed with cost-effectiveness and privacy in mind, the system runs on repurposed hardware like Intel NUCs, old set-top boxes, and commodity drives connected through local networking gear.

> âš™ï¸ A personal "AI cluster" built from reused componentsâ€”empowering local, private intelligence.

---

## ğŸ¯ Purpose

Create a **fully local, offline-capable grid** to host LLMs and personal data sources for an AI assistant. This system supports inference, document ingestion, and vector searchâ€”without relying on any external cloud infrastructure unless explicitly configured.

---

## ğŸ› ï¸ Tech Stack (Planned)

- **Docker + Docker Compose** â€“ Container orchestration
- **Ollama / vLLM / LocalAI** â€“ Lightweight, local LLM runners
- **ChromaDB / Qdrant / Weaviate** â€“ Vector DBs for embeddings
- **FastAPI / Flask** â€“ Gateway APIs for LLM and embedding access
- **Redis / NATS** â€“ Message brokering and distributed task coordination
- **Airflow / Prefect** â€“ Ingestion and data sync pipelines
- **Traefik / NGINX** â€“ Routing and TLS support
- **OpenTelemetry + Grafana** â€“ Monitoring and observability
- **Tailscale / Zerotier** â€“ Optional P2P networking

---

## ğŸ§± Hardware Architecture

- **Nodes**: Intel NUCs, set-top boxes with at least 4GB RAM & 2-core CPU
- **Storage**: Local HDDs/SSDs (1â€“4 TB) per node for redundancy
- **Network**: Old consumer-grade switches with static IP or mDNS-based addressing
- **Cooling/Power**: Minimal (passive setups encouraged)

> Power-efficient, self-hosted edge computing for AI workloads.

---

## ğŸ—‚ï¸ Project Structure (Planned)

```
onprem/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ ollama/             # LLM container with models
â”‚   â”œâ”€â”€ chromadb/           # Vector DB setup
â”‚   â”œâ”€â”€ ingest/             # File & document pipelines
â”‚   â”œâ”€â”€ gateway/            # Unified API access layer
â”‚   â””â”€â”€ traefik/            # Reverse proxy and routing
â”œâ”€â”€ scripts/                # Setup and dev tooling
â”œâ”€â”€ node-configs/           # Host-specific settings
â”œâ”€â”€ config/                 # Model configs, secrets, etc.
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ”§ Features

| Feature                                | Status  | Notes |
|----------------------------------------|---------|-------|
| Distributed model runner grid          | ğŸ”² Todo | Multi-node Ollama or vLLM |
| Shared vector DB for knowledge queries | ğŸ”² Todo | ChromaDB with ingestion |
| File & doc ingestion pipelines         | ğŸ”² Todo | PDFs, .txt, .md files |
| Private LLM gateway API                | ğŸ”² Todo | For frontend/backend access |
| P2P node discovery                     | ğŸ”² Todo | Tailscale, Zerotier optional |
| Monitoring and telemetry               | ğŸ”² Todo | Resource use, uptime, queue status |
| Offline-first design                   | âœ… Base | Core premise of system |

---

## ğŸš€ Getting Started

1. **Clone the repo**

```bash
git clone https://github.com/your-username/personal-ai-assistant-onprem.git
cd personal-ai-assistant-onprem
```

2. **Create `.env` file**

```bash
cp .env.example .env
# Edit with model paths, ports, secrets
```

3. **Start the stack**

```bash
docker-compose up --build
```

Each node will run a subset of services and sync to a shared vector or messaging layer.

---

## ğŸ” Design Principles

- **Privacy by default** â€“ All inference and data stays local
- **Reuse and recycle** â€“ Built to run on old hardware
- **Modular and maintainable** â€“ Services are containerized
- **Scalable locally** â€“ Add more nodes for redundancy or power

---

## ğŸ“… Roadmap

- [ ] Compose + Swarm-ready service layout
- [ ] Ollama + ChromaDB integration
- [ ] Embedding and QA endpoint
- [ ] Local notes/document indexer
- [ ] Web dashboard for cluster status
- [ ] Offline model auto-download manager
- [ ] External access via private gateway (optional)

---

## ğŸ“œ License

MIT License â€“ make it your own and keep it local.

---

> ğŸ§© Reimagining smart assistants with low-cost edge computing and total control.
